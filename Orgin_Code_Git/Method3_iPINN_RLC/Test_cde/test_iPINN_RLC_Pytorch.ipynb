{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/inverse-physics-informed-neural-net-3b636efeb37e\n",
    "\n",
    "https://github.com/w-wojtak/PINNs-and-iPINNs-Pytorch/blob/main/iPINN_RLC_Pytorch.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, parameters):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        [nn_layers, act_fun] = parameters\n",
    "\n",
    "        # Define a dictionary for activation functions\n",
    "        af_list = {\n",
    "            'tanh': nn.Tanh(),\n",
    "            'sigmoid': nn.Sigmoid(),\n",
    "            'relu': nn.ReLU(),\n",
    "            'gelu': nn.GELU()\n",
    "        }\n",
    "        self.activation_function = af_list.get(act_fun, None)\n",
    "\n",
    "        # Check if activation function is provided\n",
    "        if self.activation_function is None:\n",
    "            raise ValueError(f\"Activation function '{act_fun}' is not supported.\")\n",
    "\n",
    "        # Create layers dynamically based on nn_layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(nn_layers) - 1):\n",
    "            self.layers.append(nn.Linear(nn_layers[i], nn_layers[i + 1]))\n",
    "            if i < len(nn_layers) - 2:  # No activation function after the last layer\n",
    "                self.layers.append(self.activation_function)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(net_model, t):\n",
    "    model_return = net_model(t)\n",
    "    return model_return\n",
    "\n",
    "def squared_difference(input, target):\n",
    "    return (input - target) ** 2\n",
    "\n",
    "# t_range, R, L, C, VC_0 = ode_parameters  \n",
    "def calculate_analytical_solution(ode_parameters, scenario, ni):\n",
    "    t_range, _, _, _, VC_0 = ode_parameters\n",
    "    i_calc = np.zeros(ni)\n",
    "    t_test = np.linspace(t_range[0], t_range[1], ni)\n",
    "\n",
    "    R = 1.2 # resistance, ohm\n",
    "    L = 1.5 # inductance, H\n",
    "    C = 0.3 # capacitance, F\n",
    "    VC_0 = 12.0 # volts, initial capacitor voltage\n",
    "    alpha = R / (2 * L)\n",
    "\n",
    "    for i in range(ni):\n",
    "        i_calc[i] = 5.57 * np.exp(-0.4 * t_test[i]) * np.sin(1.44 * t_test[i])\n",
    "\n",
    "    return t_test, i_calc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(ode_parameters, train_parameters, nn_parameters, t_f, u_test_pred):\n",
    "    # loading parameters\n",
    "    [scenario, ni, opt, max_epochs, min_loss, learning_rate] = train_parameters\n",
    "    [t_range, R, L, C, VC_0] = ode_parameters\n",
    "\n",
    "    # initial condition - same as for PINN\n",
    "    u_0 = [0.0];\n",
    "    t_i = torch.FloatTensor(np.array(t_range[0]).reshape(-1, 1)) #torch.FloatTensor([[0.0]])\n",
    "    u_i = torch.FloatTensor(np.array(u_0).reshape(-1, 1)) #torch.FloatTensor([[0.0]])\n",
    "\n",
    "    #  f'(t) initial conditions - same as for PINN\n",
    "    u_i2 = torch.FloatTensor(np.array(VC_0).reshape(-1, 1)) #u_i2: tensor([[12.]])\n",
    "\n",
    "    t_i.requires_grad = True\n",
    "    t_f.requires_grad = True\n",
    "\n",
    "    net_model = NeuralNetwork(nn_parameters)\n",
    "\n",
    "    train_R = torch.nn.Parameter(torch.FloatTensor([R]), requires_grad=True)\n",
    "    net_model.register_parameter('R', train_R)\n",
    "\n",
    "    train_L = torch.nn.Parameter(torch.FloatTensor([L]), requires_grad=True)\n",
    "    net_model.register_parameter('L', train_L)\n",
    "\n",
    "    train_C = torch.nn.Parameter(torch.FloatTensor([C]), requires_grad=True)\n",
    "    net_model.register_parameter('C', train_C)\n",
    "\n",
    "    # choose optimizer\n",
    "    if opt == 1:\n",
    "        optimizer = torch.optim.Adam([{'params': net_model.parameters()}], lr=learning_rate)\n",
    "    elif opt == 2:\n",
    "        optimizer = torch.optim.SGD([{'params': net_model.parameters()}], lr=learning_rate)\n",
    "    else:\n",
    "        optimizer = torch.optim.LBFGS([{'params': net_model.parameters()}], lr=learning_rate)\n",
    "\n",
    "    epoch = 0\n",
    "    loss = 10\n",
    "    loss_record = np.empty([0, 3])\n",
    "    rlc_record = np.empty([0, 3])\n",
    "    plt.ion()\n",
    "    print('------------------------Neural network------------------------------------')\n",
    "    print(net_model)\n",
    "    print('----------------------------Optimizer--------------------------------------')\n",
    "    print(optimizer)\n",
    "    #  -----------   start training   ------------\n",
    "    starttime_train = datetime.datetime.now()\n",
    "    formatted_time = starttime_train.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print('------------------------Start training:{}---------------------'.format(formatted_time))\n",
    "\n",
    "    while epoch < max_epochs and loss > min_loss:\n",
    "        def closure():\n",
    "            # u_i refers to the initial condition, u_f to the solution\n",
    "            u_i_pred = pred(net_model, t_i) #초기조건 t_i에서 net_model을 사용해 예측\n",
    "            u_f_pred = pred(net_model, t_f) #도메인 내 임의의 점 t_f에서 net_model을 사용해 예측\n",
    "\n",
    "            u_i_pred_dt = torch.autograd.grad(u_i_pred.sum(), t_i, create_graph=True)[0] #초기조건에서의 예측값에 대한 시간 미분\n",
    "            u_f_pred_dt = torch.autograd.grad(u_f_pred.sum(), t_f, create_graph=True)[0] # 도메인 내 임의의 점에서 예측값에 대한 시간 미분\n",
    "            u_f_pred_dtt = torch.autograd.grad(u_f_pred_dt.sum(), t_f, create_graph=True)[0] #도메인 내 임의의 점에서 예측값에 대한 2차 시간 미분\n",
    "\n",
    "            f = (train_L * u_f_pred_dtt) + (train_R * u_f_pred_dt) + (u_f_pred / train_C)\n",
    "            \n",
    "            # get the three loss components\n",
    "            loss_1 = torch.mean(squared_difference(u_i_pred, u_i))\n",
    "            loss_2 = torch.mean(squared_difference(f, torch.zeros_like(t_f)))\n",
    "            loss_3 = torch.mean(squared_difference(u_i_pred_dt, (u_i2 / train_L)))\n",
    "            loss_4 = torch.mean(squared_difference(u_f_pred, u_test_pred))\n",
    "\n",
    "            loss_total = loss_1 + loss_2 + loss_3 + loss_4\n",
    "            #TODO!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = ['./model', './data', './figures']\n",
    "for dir in dirs:\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "scenario = 0 # 0: under-damped, 1: critically-damped, 2: over-damped\n",
    "\n",
    "# Load data\n",
    "base_filename = './data/pinn_data'\n",
    "\n",
    "# Determine the filename based on the scenario\n",
    "if scenario == 0:\n",
    "    filename = base_filename + '_under-damped.pt'\n",
    "elif scenario == 1:\n",
    "    filename = base_filename + '_critically-damped.pt'\n",
    "elif scenario == 2:\n",
    "    filename = base_filename + '_over-damped.pt'\n",
    "else:\n",
    "    raise ValueError(\"Invalid scenario value. Scenario should be 0, 1, or 2.\")\n",
    "\n",
    "# Load the tensor from the file\n",
    "pinn_data = torch.load(filename)\n",
    "\n",
    "# Extract t_f and u_test_pred from the loaded tensor\n",
    "# t_f : time\n",
    "# u_test_pred\n",
    "t_f, u_test_pred = pinn_data[:, 0], pinn_data[:, 1]\n",
    "#pinn_data.size() -> torch.Size([50, 2])\n",
    "t_f = t_f.unsqueeze(1)\n",
    "u_test_pred = u_test_pred.unsqueeze(1)\n",
    "\n",
    "t_range = [0.0, 6.0]\n",
    "VC_0 = 12.0\n",
    "R, L, C = 0.1, 0.1, 0.1\n",
    "ode_parameters = [t_range, R, L, C, VC_0]\n",
    "\n",
    "ni = 50\n",
    "optimizer = 0  # 0: L-BFGS 1: Adam 2: SGD\n",
    "max_epochs = 5000\n",
    "min_loss = 1e-8\n",
    "learning_rate = 0.01\n",
    "train_parameters = [scenario, ni, optimizer, max_epochs, min_loss, learning_rate]\n",
    "\n",
    "test_parameters = [scenario, ni]\n",
    "\n",
    "# Neural networks parameters\n",
    "nn_layers = [1, 128, 128, 1]  # neural networks layers\n",
    "act_fun = 'gelu'\n",
    "nn_parameters = [nn_layers, act_fun]\n",
    "\n",
    "train(ode_parameters, train_parameters, nn_parameters, t_f, u_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
