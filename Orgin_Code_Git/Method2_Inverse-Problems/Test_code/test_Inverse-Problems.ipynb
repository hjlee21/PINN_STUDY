{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# u(x,y) = (1-x^2-y^2)/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input layer size: 2 -> 50\n",
      "Hidden layers configuration: Sequential(\n",
      "  (0): Tanh()\n",
      "  (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "  (2): Tanh()\n",
      "  (3): Linear(in_features=50, out_features=50, bias=True)\n",
      ")\n",
      "Output layer size: 50 -> 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import Delaunay\n",
    "\n",
    "\n",
    "class PINN(nn.Module):\n",
    "    def __init__(self, num_neurons):\n",
    "        super(PINN, self).__init__()\n",
    "        self.input_layer = nn.Linear(2, num_neurons)\n",
    "        # print(f\"Input layer size: {self.input_layer.in_features} -> {self.input_layer.out_features}\")\n",
    "\n",
    "        self.hidden_layers = nn.Sequential(\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(num_neurons, num_neurons),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(num_neurons, num_neurons)\n",
    "        )\n",
    "        # print(f\"Hidden layers configuration: {self.hidden_layers}\")\n",
    "\n",
    "        self.output_layer = nn.Linear(num_neurons, 1)\n",
    "        # print(f\"Output layer size: {self.output_layer.in_features} -> {self.output_layer.out_features}\")\n",
    "\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.input_layer(x))\n",
    "        x = self.hidden_layers(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "def create_circle_mesh(radius=1.0, hmax=0.05):\n",
    "    theta = np.linspace(0, 2*np.pi, int(2*np.pi/hmax))\n",
    "    points = np.array([radius * np.cos(theta), radius * np.sin(theta)]).T\n",
    "    tri = Delaunay(points)\n",
    "    return points, tri\n",
    "\n",
    "pdeCoeffs = {'c': 0.5, 'm': 0, 'd': 0, 'a': 0, 'f': 1}\n",
    "\n",
    "points, tri = create_circle_mesh()\n",
    "boundary_nodes = np.where(np.linalg.norm(points, axis=1) == 1.0)[0]\n",
    "domain_nodes = np.setdiff1d(np.arange(len(points)), boundary_nodes)\n",
    "\n",
    "numNeurons = 50\n",
    "pinn = PINN(numNeurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 23 is out of bounds for axis 0 with size 23",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(domain_nodes), miniBatchSize): \u001b[38;5;66;03m#미니 배치 크기만큼 반복함\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     indices \u001b[38;5;241m=\u001b[39m permutation[i:i \u001b[38;5;241m+\u001b[39m miniBatchSize] \u001b[38;5;66;03m#현재 미니 배치의 인덱스를 가져옴\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     batch_nodes \u001b[38;5;241m=\u001b[39m \u001b[43mpoints\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdomain_nodes\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;66;03m# 미니 배치의 노드를 선택\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batch_nodes) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 23 is out of bounds for axis 0 with size 23"
     ]
    }
   ],
   "source": [
    "# Training setup\n",
    "numEpochs = 1500\n",
    "miniBatchSize = 2**12\n",
    "initialLearnRate = 0.01 #초기 학습률\n",
    "learnRateDecay = 0.001  #학습률 감쇠 계수\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(pinn.parameters(), lr=initialLearnRate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(numEpochs):\n",
    "    permutation = np.random.permutation(domain_nodes) #도메인 노드의 인덱스를 무작위로 섞음\n",
    "\n",
    "    for i in range(0, len(domain_nodes), miniBatchSize): #미니 배치 크기만큼 반복함\n",
    "        indices = permutation[i:i + miniBatchSize] #현재 미니 배치의 인덱스를 가져옴\n",
    "        batch_nodes = points[domain_nodes][indices] # 미니 배치의 노드를 선택\n",
    "\n",
    "        if len(batch_nodes) == 0:\n",
    "            continue\n",
    "        XY = torch.tensor(batch_nodes, dtype=torch.float32) #미니 배치 노드를 텐서로 변환함\n",
    "\n",
    "        # Forward pass\n",
    "        U = pinn(XY)\n",
    "\n",
    "        # Compute the loss\n",
    "        Utrue = (1 - XY[:, 0]**2 - XY[:, 1]**2) / 4 #실제값을 계산\n",
    "        lossData = loss_fn(U, Utrue.unsqueeze(1))\n",
    "\n",
    "        # Compute gradients\n",
    "        gradU = torch.autograd.grad(outputs=U.sum(), inputs=XY, create_graph=True)[0] #U에 대한 XY의 그래디언트를 계산\n",
    "        Laplacian = torch.zeros_like(U) #라플라시안을 0으로 초기화\n",
    "        for j in range(2): # 각 차원에 대해 반복\n",
    "            gradU2 = torch.autograd.grad(outputs=(pdeCoeffs['c'] * gradU[:, j]).sum(), inputs=XY, create_graph=True)[0]\n",
    "            # 각 차원에 대한 2차 그래디언트 계산\n",
    "            Laplacian += gradU2[:, j].unsqueeze(1)\n",
    "            # 라플라시안에 2차 그래디언트를 더함\n",
    "\n",
    "        # Enforce PDE\n",
    "        res = -pdeCoeffs['f'] - Laplacian + pdeCoeffs['a'] * U # PDE를 적용한 결과를 계산\n",
    "        lossF = torch.mean(res**2) # PDE손실 계산\n",
    "\n",
    "        # Enforce boundary conditions\n",
    "        BC_XY = torch.tensor(points[boundary_nodes], dtype=torch.float32) #경계 조건을 적용할 노드를 텐서로 변환\n",
    "        actualBC = torch.zeros(BC_XY.shape[0], 1) #실제 경계 조건 값을 0으로 설정\n",
    "        predictedBC = pinn(BC_XY) #모델을 통해 경계 조건의 예측값을 계산\n",
    "        lossBC = loss_fn(predictedBC, actualBC) #예측 경계 조건과 실제 경계 조건 사이의 손실을 계산\n",
    "\n",
    "        # Combine weighted losses\n",
    "        lambdaPDE = 0.4\n",
    "        lambdaBC = 0.6\n",
    "        lambdaData = 0.5\n",
    "        loss = lambdaPDE * lossF + lambdaBC * lossBC + lambdaData * lossData\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Learning rate decay\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = initialLearnRate / (1 + learnRateDecay * (epoch + 1))\n",
    "\n",
    "    # Print the loss for every epoch\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{numEpochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
